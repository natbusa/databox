{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalabframework\n",
    "\n",
    "The datalabframework is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf\n",
    "from datalabframework import logging as log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "One of the main things here is to have configuration and code separated in different files. Project is all about setting the correct working directories where to run and find your notebooks, python files and configuration files. When the datalabframework project is loaded, it starts by searching for a `__main__.py` file, according to python module file naming conventions. When such a file is found, the corresponding directory is set as the root path for the project. All modules and alias paths are all relative to the project root path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "Logging can be configured via metadata.yml file. The logging section of the metadata will allow you to define three types of handlers: a stdout handler, a file handler, and a kafka handler. Here below the configuration details:\n",
    "\n",
    "```\n",
    "loggers:\n",
    "    root:\n",
    "        severity: info\n",
    "\n",
    "    datalabframework:\n",
    "        name: dlf\n",
    "        stdio:\n",
    "            enable: true\n",
    "            severity: notice\n",
    "        file:\n",
    "            enable: true\n",
    "            severity: notice\n",
    "        kafka:\n",
    "            enable: false\n",
    "            severity: info\n",
    "            hosts:\n",
    "                kafka-node1:9092\n",
    "                kafka-node2:9092\n",
    "            topic: dlf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "\n",
    "Logging via the datalabframework support 5 levels:\n",
    "  - info\n",
    "  - notice\n",
    "  - warning\n",
    "  - error\n",
    "  - fatal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No project metadata loaded.\n",
    "Logging will work without loading any metadata project configuration, but in this case it will use the default cofiguration of the python root logger. By default, `debug`, `info` and `notice` level are filtered out. To enable the full functionality, including logging to kafka and logging the custom logging information about the project (sessionid, username, etc) you must load a project first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a warning message\n",
      "this is an error\n",
      "critical condition\n"
     ]
    }
   ],
   "source": [
    "log.debug('debug')\n",
    "log.info('notice')\n",
    "log.notice('jnotice')\n",
    "log.warning('a warning message')\n",
    "log.error('this is an error')\n",
    "log.critical('critical condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a metadata profile\n",
    "If a logging configuration is loaded, then extra functionality will be available. In particular, logging will log datalabframework specific info, such as the session id, and data can be passed as a dictionary, optionally with a custom message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7f3de4105be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE:dlf:run_code hello world\n"
     ]
    }
   ],
   "source": [
    "# custom message\n",
    "dlf.logging.notice('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:dlf:run_code data\n"
     ]
    }
   ],
   "source": [
    "# custom data\n",
    "dlf.logging.warning({'test_value':42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:dlf:run_code custom message\n"
     ]
    }
   ],
   "source": [
    "# custom data and message\n",
    "dlf.logging.warning('custom message', extra={'more':123})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE:dlf:my_function data\n",
      "WARNING:dlf:my_nested_function another message\n",
      "ERROR:dlf:my_nested_function custom\n"
     ]
    }
   ],
   "source": [
    "# from a function\n",
    "\n",
    "def my_nested_function():\n",
    "    log.warning('another message')\n",
    "    log.error('custom',extra=[1,2,3])\n",
    "    \n",
    "def my_function():\n",
    "    log.notice({'a':'text', 'b':2})\n",
    "    my_nested_function()\n",
    "    \n",
    "my_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
